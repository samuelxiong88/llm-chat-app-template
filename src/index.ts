/**
 * LLM Chat App on Cloudflare Workers â€” OpenAI backend (final)
 * - /api/debug  æŸ¥çœ‹ç¯å¢ƒå˜é‡
 * - /api/ping   GET /v1/models éªŒè¯ KEY/ç½‘ç»œ
 * - /api/chat   /v1/chat/completionsï¼ˆæ”¯æŒ POST æ­£å¼ + GET è°ƒè¯• ?q=ï¼‰
 *   å°† OpenAI çš„ SSE è§£æå¹¶è½¬ä¸ºå‰ç«¯åè®®ï¼š
 *     data: {"response":"<ç´¯è®¡å…¨æ–‡>","done":false}\n\n
 *     data: {"done":true}\n\n
 */

import type { Env, ChatMessage } from "./types";

const SYSTEM_PROMPT =
  "You are a helpful, friendly assistant. Provide concise and accurate responses.";

const DEFAULT_API_BASE = "https://api.openai.com/v1";

// GPT-5 / realtime / audio ç³»åˆ—ä¸æ¥å—è‡ªå®šä¹‰ temperature
function modelSupportsTemperature(model: string): boolean {
  const m = (model || "").toLowerCase();
  return !(m.startsWith("gpt-5") || m.includes("realtime") || m.includes("audio"));
}

function jsonResponse(obj: unknown, status = 200): Response {
  return new Response(JSON.stringify(obj), {
    status,
    headers: { "content-type": "application/json", "Access-Control-Allow-Origin": "*" },
  });
}

export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    try {
      const url = new URL(request.url);
      const apiBase = env.OPENAI_API_BASE || DEFAULT_API_BASE;

      // ===== é™æ€èµ„æºï¼ˆå‰ç«¯ï¼‰â€” å¸¦å…œåº•ï¼Œé¿å… ASSETS æœªç»‘å®šæŠ› 1101 =====
      if (url.pathname === "/" || !url.pathname.startsWith("/api/")) {
        try {
          if (env.ASSETS && typeof (env.ASSETS as any).fetch === "function") {
            return env.ASSETS.fetch(request);
          }
        } catch (e) {
          // fall through to fallback html
        }
        const html = `<!doctype html><meta charset="utf-8">
<title>LLM Chat</title>
<body style="font-family:system-ui;margin:40px">
<h2>LLM Chat App</h2>
<p>Assets binding <code>ASSETS</code> not configured. API endpoints:</p>
<ul><li><code>/api/ping</code></li><li><code>/api/chat</code></li><li><code>/api/debug</code></li></ul>
</body>`;
        return new Response(html, { headers: { "content-type": "text/html; charset=utf-8" } });
      }

      // ===== CORS é¢„æ£€ =====
      if (request.method === "OPTIONS") {
        return new Response(null, {
          headers: {
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "content-type, authorization",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Max-Age": "86400",
          },
        });
      }

      // ===== /api/debugï¼šæŸ¥çœ‹å˜é‡æ˜¯å¦è¯»åˆ° =====
      if (url.pathname === "/api/debug") {
        return jsonResponse({
          OPENAI_API_KEY: env.OPENAI_API_KEY ? "set" : "not set",
          OPENAI_MODEL: env.OPENAI_MODEL || "not set",
          OPENAI_API_BASE: env.OPENAI_API_BASE || "not set",
        });
      }

      // ===== /api/pingï¼šéªŒè¯ KEY/ç½‘ç»œ =====
      if (url.pathname === "/api/ping") {
        try {
          const r = await fetch(`${apiBase}/models`, {
            headers: { Authorization: `Bearer ${env.OPENAI_API_KEY}` },
          });
          const text = await r.text();
          return new Response(text, {
            status: r.status,
            headers: { "content-type": "application/json", "Access-Control-Allow-Origin": "*" },
          });
        } catch (e) {
          return jsonResponse({ error: String(e) }, 500);
        }
      }

      // ===== /api/chatï¼šæ”¯æŒ POSTï¼ˆæ­£å¼ï¼‰ä¸ GETï¼ˆè°ƒè¯• ?q=ï¼‰=====
      if (url.pathname === "/api/chat") {
        if (request.method === "GET") {
          // ä»…è°ƒè¯•ï¼šGET /api/chat?q=Hello
          const q = url.searchParams.get("q") || "Hello";
          const fake = new Request(request.url, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ messages: [{ role: "user", content: q }] }),
          });
          return handleChat(fake, env);
        }
        if (request.method !== "POST") return jsonResponse({ error: "Method not allowed" }, 405);
        return handleChat(request, env);
      }

      return jsonResponse({ error: "Not found" }, 404);
    } catch (e) {
      // é¡¶å±‚å…œåº•ï¼Œé¿å… 1101 HTML
      return jsonResponse({ error: "Worker exception", detail: String(e) }, 500);
    }
  },
} satisfies ExportedHandler<Env>;

async function handleChat(request: Request, env: Env): Promise<Response> {
  const apiBase = "https://api.openai.com/v1";
  const model   = env.OPENAI_MODEL || "gpt-4o";
  const body    = await request.json();

  const upstream = await fetch(`${apiBase}/chat/completions`, {
    method: "POST",
    headers: {
      "Authorization": `Bearer ${env.OPENAI_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model,
      messages: body.messages || [],
      stream: true
    }),
  });

  // ğŸ”‘ ç›´æ¥æŠŠ OpenAI çš„æµè¿”å›ç»™å‰ç«¯ï¼Œä¸åšäºŒæ¬¡è§£æ
  return new Response(upstream.body, {
    headers: {
      "Content-Type": "text/event-stream; charset=utf-8",
      "Cache-Control": "no-cache",
      "Connection": "keep-alive",
      "Access-Control-Allow-Origin": "*",
    },
  });
}

    // ç»„è£… payloadï¼ˆgpt-5* ä¸å‘ temperatureï¼‰
    const payload: any = { model, messages, stream: !debugJson };
    if (modelSupportsTemperature(model)) payload.temperature = 0.2;

    // éæµå¼ï¼ˆè°ƒè¯•ï¼‰
    if (debugJson) {
      const r = await fetch(`${apiBase}/chat/completions`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${env.OPENAI_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify(payload),
      });
      const text = await r.text();
      return new Response(text, {
        status: r.status,
        headers: { "content-type": "application/json", "Access-Control-Allow-Origin": "*" },
      });
    }

    // æµå¼
    let upstream: Response;
    try {
      upstream = await fetch(`${apiBase}/chat/completions`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${env.OPENAI_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify(payload),
      });
    } catch (e) {
      return jsonResponse({ error: "OpenAI fetch failed", detail: String(e) }, 500);
    }

    if (!upstream.ok || !upstream.body) {
      const detail = await upstream.text().catch(() => "");
      return jsonResponse({ error: "OpenAI upstream error", status: upstream.status, detail }, upstream.status);
    }

    // è§£æ OpenAI SSE â†’ è½¬ä¸ºå‰ç«¯åè®®ï¼ˆç´¯è®¡æ¨é€ + å¿…å‘ done:true + å†²æ´—å°¾å—ï¼‰
    const encoder = new TextEncoder();
    const decoder = new TextDecoder();
    const reader = upstream.body.getReader();

    let buffer = "";
    let acc = "";        // ç´¯è®¡åˆ°å½“å‰å®Œæ•´æ–‡æœ¬
    let sentAny = false; // é¦–å—å»å‰å¯¼ç©ºæ ¼

    const stream = new ReadableStream<Uint8Array>({
      async pull(controller) {
        // âœ… å°±åœ¨è¿™é‡Œï¼Œä¸€å¼€å§‹å…ˆå‘ä¸€æ¡ debug äº‹ä»¶
    controller.enqueue(
      encoder.encode(`data: ${JSON.stringify({ debug: "stream started" })}\n\n`)
    );

    const { value, done } = await reader.read();
    // ... åé¢çš„è§£æé€»è¾‘ ...

        if (done) {
          // å†²æ´—å°¾å—ï¼šbuffer é‡Œå¯èƒ½æ®‹ç•™ä¸€ä¸ªæœªä»¥ç©ºè¡Œç»“å°¾çš„äº‹ä»¶
          const tail = buffer.trim();
          if (tail.startsWith("data:")) {
            const payload = tail.slice(5).trim();
            if (payload !== "[DONE]") {
              try {
                const j = JSON.parse(payload);
                const chunk =
                  typeof j?.choices?.[0]?.delta?.content === "string"
                    ? j.choices[0].delta.content
                    : "";
                if (chunk.trim().length > 0) acc += chunk;
              } catch {}
            }
          }
          if (!sentAny && acc.length > 0) {
            acc = acc.replace(/^\s+/, "");
            sentAny = true;
          }
          if (acc.length > 0) {
            controller.enqueue(
              encoder.encode(`data: ${JSON.stringify({ response: acc, done: false })}\n\n`)
            );
          }
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ done: true })}\n\n`));
          controller.close();
          return;
        }

        buffer += decoder.decode(value, { stream: true });

        // å…¼å®¹ \n\n / \r\n\r\n äº‹ä»¶è¾¹ç•Œ
        let idx: number;
        while (
          (idx = buffer.indexOf("\n\n")) !== -1 ||
          (idx = buffer.indexOf("\r\n\r\n")) !== -1
        ) {
          const useCRLF =
            buffer.indexOf("\r\n\r\n") !== -1 && idx === buffer.indexOf("\r\n\r\n");
          const sepLen = useCRLF ? 4 : 2;

          const block = buffer.slice(0, idx);
          buffer = buffer.slice(idx + sepLen);

          for (const line of block.split(/\r?\n/)) {
            const l = line.trim();
            if (!l.startsWith("data:")) continue;
            const payload = l.slice(5).trim();

            if (payload === "[DONE]") {
              if (!sentAny && acc.length > 0) {
                acc = acc.replace(/^\s+/, "");
                sentAny = true;
              }
              if (acc.length > 0) {
                controller.enqueue(
                  encoder.encode(`data: ${JSON.stringify({ response: acc, done: false })}\n\n`)
                );
              }
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ done: true })}\n\n`));
              controller.close();
              return;
            }

            let j: any;
            try {
              j = JSON.parse(payload);
            } catch {
              continue; // å¿ƒè·³æˆ–é JSON è¡Œ
            }

            const chunk =
              typeof j?.choices?.[0]?.delta?.content === "string"
                ? j.choices[0].delta.content
                : "";

            // å¿½ç•¥ç©º/ç©ºç™½å¢é‡
            if (chunk.trim().length === 0) continue;

            // ç´¯ç§¯å…¨æ–‡ + é¦–å—å»å‰å¯¼ç©ºæ ¼
            acc += chunk;
            if (!sentAny && acc.length > 0) {
              acc = acc.replace(/^\s+/, "");
              sentAny = true;
            }

            // æ¯æ¬¡æŠŠæˆªè‡³å½“å‰çš„â€œå®Œæ•´æ–‡æœ¬â€æ¨ç»™å‰ç«¯ï¼ˆé¿å… H e low / ç©ºç™½ï¼‰
            controller.enqueue(
              encoder.encode(`data: ${JSON.stringify({ response: acc, done: false })}\n\n`)
            );
          }
        }
      },
      cancel() {
        try {
          reader.cancel();
        } catch {}
      },
    });

    return new Response(stream, {
      headers: {
        "Content-Type": "text/event-stream; charset=utf-8",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "Access-Control-Allow-Origin": "*",
      },
    });
  } catch (e) {
    return jsonResponse({ error: "handleChat exception", detail: String(e) }, 500);
  }
}
