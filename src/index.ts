/**
 * Worker â†’ OpenAI Responses API (SSE)
 * ç›®æ ‡ï¼šæµè§ˆå™¨ç«‹åˆ»æ‹¿åˆ°å“åº”å¤´ï¼ˆä¸å†ç­‰ä¸Šæ¸¸è¿æ¥å®Œæˆï¼‰ï¼Œå¹¶åœ¨éœ€è¦æ£€ç´¢æ—¶ç»™å‡ºå¯è§è¿›åº¦æç¤ºã€‚
 *
 * - æ¨¡å‹ï¼šé»˜è®¤ gpt-4oï¼ˆå¯ç”¨ OPENAI_MODEL è¦†ç›–ï¼‰
 * - å·¥å…·ï¼šè‹¥ OPENAI_NATIVE_TOOLS=on ä¸”æ¨¡å‹åœ¨ç™½åå•ï¼Œé™„å¸¦ web_search_previewï¼›å¦åˆ™è‡ªåŠ¨æ— å·¥å…·
 * - å›é€€ï¼šé‡åˆ° tools/å‚æ•°ä¸æ”¯æŒçš„ 400ï¼Œè‡ªåŠ¨å‰¥æ‰ tools/ä¸æ”¯æŒå‚æ•°é‡è¯•
 * - SSEï¼šå°† Responses äº‹ä»¶è½¬è¯‘ä¸º chat-completions é£æ ¼ choices[0].delta.content
 * - è¿›åº¦ï¼šæ˜¾ç¤º â€œğŸ” æ­£åœ¨è”ç½‘æ£€ç´¢â€¦ / ğŸ“„ å·²è·å–ç»“æœï¼Œæ­£åœ¨æ•´åˆâ€¦â€ï¼›8s å¿ƒè·³ï¼›45s æ€»è¶…æ—¶
 */

const DEFAULT_API_BASE = "https://api.openai.com/v1";
const DEFAULT_MODEL = "gpt-4o";
const DEFAULT_SYSTEM_PROMPT =
  "You are a senior bilingual (ä¸­è‹±åŒè¯­) analyst and writer. When the user asks for explanations, think step-by-step but keep the final answer concise, structured, and actionable. Prefer clear headings and short lists. Add quick checks or caveats when needed. If you are unsure, say so and state your assumptions. Use simple, precise wording; avoid purple prose. é»˜è®¤ç”¨ç”¨æˆ·çš„è¯­è¨€å›ç­”ï¼›å¦‚æœç”¨æˆ·ç”¨ä¸­æ–‡ï¼Œä½ ç”¨ä¸­æ–‡å¹¶ä¿ç•™å¿…è¦çš„è‹±æ–‡æœ¯è¯­ã€‚";

const te = new TextEncoder();
const SSE_HEADERS = {
  "Content-Type": "text/event-stream; charset=utf-8",
  "Cache-Control": "no-cache",
  Connection: "keep-alive",
  "Access-Control-Allow-Origin": "*",
};

function json(obj: unknown, status = 200): Response {
  return new Response(JSON.stringify(obj), {
    status,
    headers: { "content-type": "application/json", "Access-Control-Allow-Origin": "*" },
  });
}
function sseData(o: unknown) {
  return te.encode(`data: ${JSON.stringify(o)}\n\n`);
}
function sseDone() {
  return te.encode(`data: [DONE]\n\n`);
}
function sseErrorStream(detail: string) {
  return new ReadableStream<Uint8Array>({
    start(controller) {
      controller.enqueue(
        sseData({
          id: "cmpl-error",
          object: "chat.completion.chunk",
          choices: [{ index: 0, delta: { content: detail }, finish_reason: null }],
        })
      );
      controller.enqueue(
        sseData({
          id: "cmpl-stop",
          object: "chat.completion.chunk",
          choices: [{ index: 0, delta: {}, finish_reason: "stop" }],
        })
      );
      controller.enqueue(sseDone());
      controller.close();
    },
  });
}

export default {
  async fetch(request: Request, env: any): Promise<Response> {
    try {
      const url = new URL(request.url);
      const apiBase = (env.OPENAI_API_BASE || DEFAULT_API_BASE).trim();
      const model = (env.OPENAI_MODEL || DEFAULT_MODEL).trim();
      const SYSTEM_PROMPT = env.SYSTEM_PROMPT || DEFAULT_SYSTEM_PROMPT;

      // ä»…è¿™äº›æ¨¡å‹å°è¯•å¸¦æ‰˜ç®¡æœç´¢å·¥å…·ï¼›å…¶ä½™ä¸å¸¦ï¼Œé¿å… 400
      const TOOL_MODELS = new Set([
        "gpt-4o",
        "gpt-4o-2024-11-20",
        "gpt-4o-mini",
        "gpt-4.1",
        "gpt-4.1-mini",
      ]);
      const ENABLE_TOOLS = String(env.OPENAI_NATIVE_TOOLS || "").toLowerCase() === "on";
      const DEBUG_EVENTS = String(env.DEBUG_EVENTS || "").toLowerCase() === "on";

      // å…œåº•é¡µ
      if (url.pathname === "/" || !url.pathname.startsWith("/api/")) {
        const html = `<!doctype html><meta charset="utf-8"><title>LLM Chat</title>
<body style="font-family:system-ui;margin:40px">
<h2>LLM Chat App</h2>
<ul>
  <li><code>/api/ping</code></li>
  <li><code>/api/chat?q=hello</code></li>
  <li><code>/api/debug</code></li>
</ul>
</body>`;
        return new Response(html, { headers: { "content-type": "text/html; charset=utf-8" } });
      }

      // CORS é¢„æ£€
      if (request.method === "OPTIONS") {
        return new Response(null, {
          headers: {
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "content-type, authorization",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Max-Age": "86400",
          },
        });
      }

      // è°ƒè¯•
      if (url.pathname === "/api/debug") {
        return json({
          OPENAI_API_KEY: env.OPENAI_API_KEY ? "set" : "not set",
          OPENAI_MODEL: env.OPENAI_MODEL || "not set",
          OPENAI_API_BASE: env.OPENAI_API_BASE || "not set",
          OPENAI_NATIVE_TOOLS: ENABLE_TOOLS ? "on" : "off",
          effective_model: model,
        });
      }

      // ping
      if (url.pathname === "/api/ping") {
        try {
          const r = await fetch(`${apiBase}/models`, {
            headers: { Authorization: `Bearer ${env.OPENAI_API_KEY}` },
          });
          const text = await r.text();
          return new Response(text, {
            status: r.status,
            headers: { "content-type": "application/json", "Access-Control-Allow-Origin": "*" },
          });
        } catch (e) {
          return json({ error: String(e) }, 500);
        }
      }

      // chat
      if (url.pathname === "/api/chat") {
        // 1) ç»„è£… messages
        type Msg = { role: "system" | "user" | "assistant"; content: string };
        let messages: Msg[] = [];
        if (request.method === "GET") {
          const q = url.searchParams.get("q") || "Hello";
          messages = [{ role: "system", content: SYSTEM_PROMPT }, { role: "user", content: q }];
        } else if (request.method === "POST") {
          const body = await request.json().catch(() => ({}));
          const userMsgs = Array.isArray((body as any)?.messages) ? (body as any).messages : [];
          messages = userMsgs.length ? userMsgs : [{ role: "user", content: "Hello" }];
          if (!messages.some((m) => m.role === "system")) {
            messages.unshift({ role: "system", content: SYSTEM_PROMPT });
          }
        } else {
          return json({ error: "Method not allowed" }, 405);
        }

        // 2) å‚æ•°
        const qMax = url.searchParams.get("max_tokens") ?? url.searchParams.get("max_output_tokens");
        const qSeed = url.searchParams.get("seed");
        const qT = url.searchParams.get("temperature");
        const qTP = url.searchParams.get("top_p");

        const isThinking = /thinking/i.test(model);
        const supportsSampling = !isThinking;

        const max_output_tokens =
          qMax !== null ? Number(qMax) : env.OPENAI_MAX_TOKENS ? Number(env.OPENAI_MAX_TOKENS) : 1024;

        const seed =
          qSeed !== null ? Number(qSeed) : env.OPENAI_SEED ? Number(env.OPENAI_SEED) : undefined;

        const temperature =
          qT !== null ? Number(qT) : env.OPENAI_TEMPERATURE ? Number(env.OPENAI_TEMPERATURE) : 0.7;

        const top_p =
          qTP !== null ? Number(qTP) : env.OPENAI_TOP_P ? Number(env.OPENAI_TOP_P) : 1.0;

        // 3) åŸºæœ¬ payloadï¼ˆæ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä¸è¯·æ±‚ä¸Šæ¸¸ï¼‰
        const basePayload: any = {
          model,
          input: messages,
          stream: true,
          max_output_tokens,
        };
        if (seed !== undefined && !Number.isNaN(seed)) basePayload.seed = seed;
        if (supportsSampling) {
          basePayload.temperature = temperature;
          basePayload.top_p = top_p;
        } else {
          basePayload.reasoning = { effort: "medium" };
        }
        if (ENABLE_TOOLS && TOOL_MODELS.has(model)) {
          basePayload.tools = [{ type: "web_search_preview" }]; // å¦‚æœ‰æ–°ç‰ˆå¯æ¢ web_search_preview_2025_03_11
          basePayload.tool_choice = "auto";
        }

        // 4) å…ˆè¿”å›ä¸€ä¸ªæµï¼ˆç«‹åˆ»å‘é€å“åº”å¤´ä¸èµ·å§‹å ä½ï¼‰ï¼Œåœ¨æµå†…éƒ¨å†å»è¯·æ±‚ OpenAI
        const stream = new ReadableStream<Uint8Array>({
          start(controller) {
            // 4.1 èµ·å§‹å ä½ï¼ˆè®©æµè§ˆå™¨é©¬ä¸Šæ¸²æŸ“ï¼‰
            controller.enqueue(
              sseData({
                id: "cmpl-start",
                object: "chat.completion.chunk",
                choices: [{ index: 0, delta: { role: "assistant" }, finish_reason: null }],
              })
            );
            controller.enqueue(
              sseData({
                id: "cmpl-info",
                object: "chat.completion.chunk",
                choices: [
                  {
                    index: 0,
                    delta: { content: "â€¦ æ­£åœ¨è¿æ¥ä¸Šæ¸¸ï¼ˆå¯èƒ½è§¦å‘è”ç½‘æ£€ç´¢ï¼‰" },
                    finish_reason: null,
                  },
                ],
              })
            );

            // 4.2 åœ¨æµå†…éƒ¨å¼‚æ­¥æ‹‰ OpenAI
            (async () => {
              // æ€»ä½“è¶…æ—¶ï¼Œé¿å…æ°¸è¿œæŒ‚èµ·
              const upstreamCtl = new AbortController();
              const REQUEST_TIMEOUT_MS = 45000;
              const timeoutHandle = setTimeout(() => upstreamCtl.abort("request-timeout"), REQUEST_TIMEOUT_MS);

              // å¿ƒè·³ï¼š8s æ— å¢é‡å°±ç»™æç¤º
              let lastTextTs = Date.now();
              const HEARTBEAT_MS = 8000;
              const heartbeat = setInterval(() => {
                if (Date.now() - lastTextTs > HEARTBEAT_MS) {
                  controller.enqueue(
                    sseData({
                      id: "cmpl-chunk",
                      object: "chat.completion.chunk",
                      choices: [
                        {
                          index: 0,
                          delta: { content: "ï¼ˆä»åœ¨æ£€ç´¢ä¸æ•´åˆï¼Œè¯·ç¨å€™â€¦ï¼‰" },
                          finish_reason: null,
                        },
                      ],
                    })
                  );
                  lastTextTs = Date.now();
                }
              }, HEARTBEAT_MS);

              const pushDelta = (text: string) => {
                if (!text) return;
                lastTextTs = Date.now();
                controller.enqueue(
                  sseData({
                    id: "cmpl-chunk",
                    object: "chat.completion.chunk",
                    choices: [{ index: 0, delta: { content: text }, finish_reason: null }],
                  })
                );
              };
              const pushStop = (reason = "stop") => {
                controller.enqueue(
                  sseData({
                    id: "cmpl-stop",
                    object: "chat.completion.chunk",
                    choices: [{ index: 0, delta: {}, finish_reason: reason }],
                  })
                );
                controller.enqueue(sseDone());
              };

              try {
                const headers = {
                  Authorization: `Bearer ${env.OPENAI_API_KEY}`,
                  "Content-Type": "application/json",
                  Accept: "text/event-stream",
                  // æœ‰çš„ç§Ÿæˆ·éœ€è¦åŒæ—¶å£°æ˜ responses ä¸ tools
                  "OpenAI-Beta":
                    (env.OPENAI_BETA ? String(env.OPENAI_BETA) : "responses-2024-12-17") + "; tools=v1",
                };

                // åˆæ¬¡å°è¯•
                let payload = basePayload;
                let upstream = await fetch(`${apiBase}/responses`, {
                  method: "POST",
                  headers,
                  body: JSON.stringify(payload),
                  signal: upstreamCtl.signal,
                });

                // 400 â†’ è‡ªåŠ¨å›é€€ï¼ˆtools/å‚æ•°ä¸è¢«æ”¯æŒï¼‰
                if (!upstream.ok) {
                  const firstDetail = await upstream.text().catch(() => "");
                  const lower = firstDetail.toLowerCase();
                  const toolsProblem =
                    upstream.status === 400 &&
                    ((/invalid_value/.test(lower) && /tools/.test(lower)) ||
                      (/not supported with/.test(lower) && /tool/.test(lower)) ||
                      (/unsupported/.test(lower) && /tool/.test(lower)) ||
                      /unknown tool/.test(lower) ||
                      (/param/.test(lower) && /tools/.test(lower)));
                  const badSampling =
                    upstream.status === 400 &&
                    /unsupported/.test(lower) &&
                    /(temperature|top_p)/i.test(lower);
                  const badReasoning =
                    upstream.status === 400 &&
                    /unsupported/.test(lower) &&
                    /reasoning\.effort/i.test(lower);

                  if (toolsProblem || badSampling || badReasoning) {
                    payload = { model, input: messages, stream: true, max_output_tokens };
                    if (seed !== undefined && !Number.isNaN(seed)) (payload as any).seed = seed;
                    upstream = await fetch(`${apiBase}/responses`, {
                      method: "POST",
                      headers,
                      body: JSON.stringify(payload),
                      signal: upstreamCtl.signal,
                    });
                  } else {
                    // ä»¥ SSE å½¢å¼é€ä¼ é”™è¯¯
                    controller.enqueue(
                      sseData({
                        id: "cmpl-error",
                        object: "chat.completion.chunk",
                        choices: [
                          {
                            index: 0,
                            delta: { content: `âš ï¸ Upstream ${upstream.status}: ${firstDetail.slice(0, 800)}` },
                            finish_reason: null,
                          },
                        ],
                      })
                    );
                    pushStop("stop");
                    clearInterval(heartbeat);
                    clearTimeout(timeoutHandle);
                    controller.close();
                    return;
                  }
                }

                if (!upstream.ok || !upstream.body) {
                  const detail = await upstream.text().catch(() => "");
                  controller.enqueue(
                    sseData({
                      id: "cmpl-error",
                      object: "chat.completion.chunk",
                      choices: [
                        {
                          index: 0,
                          delta: { content: `âš ï¸ Upstream error: ${detail.slice(0, 800)}` },
                          finish_reason: null,
                        },
                      ],
                    })
                  );
                  pushStop("stop");
                  clearInterval(heartbeat);
                  clearTimeout(timeoutHandle);
                  controller.close();
                  return;
                }

                // è¯»å–ä¸Šæ¸¸ SSE å¹¶è½¬è¯‘
                const reader = upstream.body.getReader();
                const decoder = new TextDecoder("utf-8");
                let buffer = "";
                let closed = false;
                let lastEvent = "";

                while (true) {
                  const { value, done } = await reader.read();
                  if (done) break;
                  buffer += decoder.decode(value, { stream: true });

                  const lines = buffer.split("\n");
                  buffer = lines.pop() || "";

                  for (const raw of lines) {
                    const line = raw.trim();
                    if (!line) continue;

                    if (line.startsWith("event:")) {
                      lastEvent = line.slice(6).trim();
                      continue;
                    }
                    if (!line.startsWith("data:")) continue;
                    const dataStr = line.slice(5).trim();

                    if (dataStr === "[DONE]") {
                      pushStop("stop");
                      closed = true;
                      break;
                    }

                    try {
                      const obj: any = JSON.parse(dataStr);
                      const type = (obj?.type || lastEvent || obj?.event || "").toString();

                      // æ–‡æœ¬å¢é‡
                      if (
                        type.endsWith(".delta") ||
                        type === "response.delta" ||
                        typeof obj.delta === "string"
                      ) {
                        const t =
                          typeof obj.delta === "string"
                            ? obj.delta
                            : typeof obj.text === "string"
                            ? obj.text
                            : typeof obj.content === "string"
                            ? obj.content
                            : (obj?.output_text?.content?.[0]?.text || "");
                        if (t) pushDelta(t);
                        continue;
                      }

                      // å·¥å…·äº‹ä»¶ â†’ å¯è§æç¤º
                      if (/tool_call\.created$/.test(type) || /tool\.(started|created)/i.test(type)) {
                        pushDelta("ğŸ” æ­£åœ¨è”ç½‘æ£€ç´¢â€¦");
                        continue;
                      }
                      if (/tool_call\.completed$/.test(type) || /tool\.(completed|finish)/i.test(type)) {
                        pushDelta("ğŸ“„ å·²è·å–ç»“æœï¼Œæ­£åœ¨æ•´åˆâ€¦");
                        continue;
                      }

                      // å®Œæˆ
                      if (
                        type.endsWith(".done") ||
                        type === "response.completed" ||
                        obj?.done === true ||
                        obj?.status === "completed"
                      ) {
                        pushStop("stop");
                        closed = true;
                        break;
                      }

                      // è°ƒè¯•ï¼šæœªçŸ¥äº‹ä»¶å¯è§åŒ–
                      if (DEBUG_EVENTS) {
                        const brief = obj?.type || type || "event";
                        pushDelta(`ï¼ˆ${brief} â€¦ï¼‰`);
                      }
                    } catch {
                      // é JSON è¡Œå¿½ç•¥
                    }
                  }

                  if (closed) break;
                }

                clearInterval(heartbeat);
                clearTimeout(timeoutHandle);
                if (!closed) pushStop("stop");
                controller.close();
              } catch (e: any) {
                clearInterval(heartbeat);
                // è¶…æ—¶/ä¸­æ–­
                if (e?.name === "AbortError" || String(e).includes("request-timeout")) {
                  controller.enqueue(
                    sseData({
                      id: "cmpl-error",
                      object: "chat.completion.chunk",
                      choices: [
                        {
                          index: 0,
                          delta: { content: "âŒ› åç«¯è¿æ¥è¶…æ—¶ï¼ˆå¯èƒ½åœ¨è°ƒèµ·è”ç½‘æ£€ç´¢æˆ–ç½‘ç»œå—é™ï¼‰ã€‚" },
                          finish_reason: null,
                        },
                      ],
                    })
                  );
                  pushStop("stop");
                  controller.close();
                  return;
                }
                // å…¶å®ƒé”™è¯¯
                controller.enqueue(
                  sseData({
                    id: "cmpl-error",
                    object: "chat.completion.chunk",
                    choices: [
                      {
                        index: 0,
                        delta: { content: `âš ï¸ Worker error: ${String(e).slice(0, 800)}` },
                        finish_reason: null,
                      },
                    ],
                  })
                );
                pushStop("stop");
                controller.close();
              }
            })();
          },
        });

        return new Response(stream, { headers: SSE_HEADERS });
      }

      return json({ error: "Not found" }, 404);
    } catch (e) {
      return json({ error: "Worker exception", detail: String(e) }, 500);
    }
  },
};
